{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_yolo(input_images_path, input_json_path, output_images_path, output_labels_path):\n",
    "\n",
    "    f = open(input_json_path)\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    # Create directories for output images and labels\n",
    "    os.makedirs(output_images_path, exist_ok=True)\n",
    "    os.makedirs(output_labels_path, exist_ok=True)\n",
    "\n",
    "    # List to store filenames\n",
    "    file_names = []\n",
    "    for filename in os.listdir(input_images_path):\n",
    "        if filename.endswith(\".png\"):\n",
    "            source = os.path.join(input_images_path, filename)\n",
    "            destination = os.path.join(output_images_path, filename)\n",
    "            shutil.copy(source, destination)\n",
    "            file_names.append(filename)\n",
    "\n",
    "    # Function to get image annotations\n",
    "    def get_img_ann(image_id):\n",
    "        return [ann for ann in data['annotations'] if ann['image_id'] == image_id]\n",
    "\n",
    "    # Function to get image data\n",
    "    def get_img(filename):\n",
    "        return next((img for img in data['images'] if img['file_name'] == filename), None)\n",
    "\n",
    "    # Iterate through filenames and process each image\n",
    "    for filename in file_names:\n",
    "        img = get_img(filename)\n",
    "        img_id = img['id']\n",
    "        img_w = img['width']\n",
    "        img_h = img['height']\n",
    "        img_ann = get_img_ann(img_id)\n",
    "\n",
    "        # Write normalized polygon data to a text file\n",
    "        if img_ann:\n",
    "            with open(os.path.join(output_labels_path, f\"{os.path.splitext(filename)[0]}.txt\"), \"a\") as file_object:\n",
    "                for ann in img_ann:\n",
    "                    current_category = ann['category_id'] - 1\n",
    "                    polygon = ann['segmentation'][0]\n",
    "                    normalized_polygon = [format(coord / img_w if i % 2 == 0 else coord / img_h, '.6f') for i, coord in enumerate(polygon)]\n",
    "                    file_object.write(f\"{current_category} \" + \" \".join(normalized_polygon) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a YAML file for the dataset\n",
    "def create_yaml(input_json_path, output_yaml_path, train_path, val_path, test_path=None):\n",
    "    with open(input_json_path) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Extract the category names\n",
    "    names = [category['name'] for category in data['categories']]\n",
    "    \n",
    "    # Number of classes\n",
    "    nc = len(names)\n",
    "\n",
    "    # Create a dictionary with the required content\n",
    "    yaml_data = {\n",
    "        'names': names,\n",
    "        'nc': nc,\n",
    "        'test': test_path if test_path else '',\n",
    "        'train': train_path,\n",
    "        'val': val_path\n",
    "    }\n",
    "\n",
    "    # Write the dictionary to a YAML file\n",
    "    with open(output_yaml_path, 'w') as file:\n",
    "        yaml.dump(yaml_data, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_input_path = \"C:/Users/000501/Desktop/Python/work/thk gifu/train/\"\n",
    "base_output_path = \"C:/Users/000501/Desktop/Python/work/thk gifu/yolo_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing training dataset \n",
    "convert_to_yolo(\n",
    "    input_images_path=os.path.join(base_input_path, \"aug_img\"),\n",
    "    input_json_path=os.path.join(base_input_path, \"aug_img/train.json\"),\n",
    "    output_images_path=os.path.join(base_output_path, \"train/images\"),\n",
    "    output_labels_path=os.path.join(base_output_path, \"train/labels\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the YAML configuration file\n",
    "create_yaml(\n",
    "    input_json_path=os.path.join(base_input_path, \"aug_img/train.json\"),\n",
    "    output_yaml_path=os.path.join(base_output_path, \"data.yaml\"),\n",
    "    train_path=\"EM-Platelet/train/images\",\n",
    "    val_path=\"EM-Platelet/valid/images\",\n",
    "    test_path='../test/images'  # or None if not applicable\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
